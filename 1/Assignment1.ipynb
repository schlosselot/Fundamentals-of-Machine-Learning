{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "36eae6af6a8b8707f14906e588a3ee64",
     "grade": false,
     "grade_id": "cell-a5d8c8596524d405",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# ELEC 400M / EECE 571M Assignment 1: Linear models for classification\n",
    "(This assignment is a modified version of an assignment used in ECE 421 at the University of Toronto and kindly made available to us by the instructor.)\n",
    "\n",
    "In this assignment, you will be using linear models discussed in the lectures to perform a binary classification task. You will compare the performances of linear classification and logistic regression using suitable training algorithms. The implementation will be done in python using functions from the NumPy library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a9a8b3c4101c3b8441a31e218145894f",
     "grade": false,
     "grade_id": "cell-6a4eff1ae721ca9a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Data Set\n",
    "We consider the dataset of images of letters contained in file notMNIST.npz. In particular, you will use a smaller dataset that only contains the images from two letter classes: “C” (the positive class) and “J” (the negative class). The images are of size 28 × 28 pixels. The figure below shows 20 randomly selected image samples for the letters “C” and “J”.\n",
    "\n",
    "![](sample_images.eps)\n",
    "\n",
    "You will apply the function `loadData` to generate the subset of images containing only letters “C” and “J”. This script organizes the total set of 3,745 images into maller subsets containing 3,500 training images, 100 validation images and 145 test images. Their use will be further specified in the problem descriptions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "734015703ccb840f57bb43468c573784",
     "grade": false,
     "grade_id": "cell-eef20adb07056077",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a07dc59b87c2b66db4da8655d72e3bf1",
     "grade": false,
     "grade_id": "cell-9d51b6e8a7c666a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    with np.load('notMNIST.npz') as data:\n",
    "        Data, Target = data['images'], data['labels']\n",
    "        posClass = 2\n",
    "        negClass = 9\n",
    "        dataIndx = (Target==posClass) + (Target==negClass)\n",
    "        Data = Data[dataIndx]/255.0\n",
    "        Target = Target[dataIndx].reshape(-1, 1)\n",
    "        Target[Target==posClass] = 1\n",
    "        Target[Target==negClass] = 0\n",
    "        np.random.seed(1)\n",
    "        randIndx = np.arange(len(Data))\n",
    "        np.random.shuffle(randIndx)\n",
    "        Data, Target = Data[randIndx], Target[randIndx]\n",
    "        trainData, trainTarget = Data[:3500], Target[:3500]\n",
    "        validData, validTarget = Data[3500:3600], Target[3500:3600]\n",
    "        testData, testTarget = Data[3600:], Target[3600:]\n",
    "       \n",
    "    return trainData, validData, testData, trainTarget, validTarget, testTarget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8f087958f088ae0145d1e68b806545c8",
     "grade": false,
     "grade_id": "cell-5b5bf0983e23b493",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Linear Classification\n",
    "\n",
    "The first classifier is the linear classifier \n",
    "$$\\hat{y}=\\mathrm{sign}\\left(\\sum_{i=0}^dw_ix_i\\right)\\,$$\n",
    "where $x_0=1$ so that $b=w_0x_0$ is the bias term and $x_1,\\ldots, x_d$ are the input features.\n",
    "The loss function for an input-output pair $(\\underline{x}_n,y_n)$ and given model parameters $\\underline{w}$ is \n",
    "$$L_n(\\underline{w})= \\mathbf{1}\\{\\hat{y}_n\\neq y_n\\}\\;.$$\n",
    "The total loss for $N$ samples is \n",
    "$$L(\\underline{w})= \\frac{1}{N}\\sum\\limits_{n=1}^N\\mathbf{1}\\{\\hat{y}_n\\neq y_n\\}\\;.$$\n",
    "\n",
    "\n",
    "\n",
    "### Notes on Classification\n",
    "* The classification should be based on the $d=28\\times 28=784$ intensity values in an image. This means that you need to flatten the 2D images to 1D input vectors $\\underline{x}$ of length 784.\n",
    "* The outputs $\\hat{y}$ of the perceptron model are from $\\{-1,+1\\}$, while the target variable from the data set is from $\\{0,1\\}$. You need to make adjustements to account for this difference, which can include adjusting the data type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6af4e6b0785c4453b46bf6443a37c31b",
     "grade": false,
     "grade_id": "cell-39aa5707539633da",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Loss Function [2 points]\n",
    "\n",
    "Implement a function to compute the classification loss as defined above. The function has three input arguments: the weight vector, the feature vectors, and the labels. It returns the total loss associated with the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "818a490ac40937e4f87242012417534a",
     "grade": true,
     "grade_id": "cell-ae171a05a9de28c1",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ErrorRate (w, x, y):\n",
    "    \n",
    "    #iterate through input sets to caculate error uisng indicator function\n",
    "    errSum = 0       #error accumulator\n",
    "    for i in range(len(x)):\n",
    "        \n",
    "        #calculate inner product of <weights,inputs>\n",
    "        y_hat = np.sign(np.dot(w,x[i]))\n",
    "        \n",
    "        #accumulate error using indicator function\n",
    "        errSum += 1*(y_hat != y[i])\n",
    "        \n",
    "    return errSum / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b9b449fbdd732dea70062ea3abde3e73",
     "grade": false,
     "grade_id": "cell-34e3d06afbbedc4e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Perceptron Learning Algorithm [10 points]\n",
    "\n",
    "Implement a function for the perceptron learning algorithm (PLA) which accepts four arguments: an inital weight vector, the data, the labels, and the maximal number of iterations it executes. It is thus a version of the PLA is assured to terminate. The function returns the updated weight vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cfbe75572c9845886002b30c30286e90",
     "grade": true,
     "grade_id": "cell-8968d65ea48b9d30",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def PLA(w, x, y, maxIter):\n",
    "     \n",
    "    iterations = 0\n",
    "    errorFlag = True;  #Flag to efficiently indicate error without calling ErrorRate(). \n",
    "    while((errorFlag == True) and (iterations < maxIter)):\n",
    "        \n",
    "        iterations += 1     \n",
    "        errorFlag = False\n",
    "        i_x_misclassified = []\n",
    "        \n",
    "        #iterate through input sets \n",
    "        for i in range(len(y)):   \n",
    "            \n",
    "            #predict output: calculate inner product of <weights,inputs>\n",
    "            y_hat = np.sign(np.dot(w,x[i]))\n",
    "            \n",
    "            #check prediction against target output; if an error is detected, add the error index to a list                  \n",
    "            if (y_hat != y[i]):\n",
    "                i_x_misclassified.append(i)\n",
    "                errorFlag = True;\n",
    "        \n",
    "        #randomly choose one error and correct it \n",
    "        if errorFlag == True: \n",
    "            i = i_x_misclassified[np.random.randint(len(i_x_misclassified))]\n",
    "            y_hat = np.sign(np.dot(w,x[i]))\n",
    "            if (y_hat < y[i]):\n",
    "                w += x[i]\n",
    "            else:\n",
    "                w -= x[i]\n",
    "    #end while loop\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "27aa2ab61179ae69bfc9d3a3b4913d73",
     "grade": false,
     "grade_id": "cell-71e4ba41ec059bc8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test the `PLA` function by training the classifier on the training data (`trainData`, `trainTarget`) with a maximum of 100 iterations, and measuring the cassification error using the testing data (`testData`, `testTarget`). Write the test script into the box below and let it print the classiciation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "97993999c368017a94b44290429d704a",
     "grade": true,
     "grade_id": "cell-f6ac3ead6dede7dc",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02758621]\n"
     ]
    }
   ],
   "source": [
    "trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n",
    "\n",
    "#flatten x matrix from 28x28 to 784x1 vector and add bias to each input set\n",
    "x_train = [k.flatten() for k in trainData]\n",
    "x_train = np.hstack( (np.vstack(np.ones(len(x_train))), x_train) )\n",
    "\n",
    "x_test = [k.flatten() for k in testData]\n",
    "x_test = np.hstack( (np.vstack(np.ones(len(x_test))), x_test) )\n",
    "\n",
    "#re-scale y vector from 0->1 to -1->+1\n",
    "y_train = [-1+2*k for k in trainTarget]\n",
    "y_test = [-1+2*k for k in testTarget]\n",
    "\n",
    "#seed the randomization for the PLA for comparison with the first PLA test (same seed used, thus it resets)\n",
    "np.random.seed(1)\n",
    "\n",
    "#intialize weight vector with zero vector\n",
    "w = np.zeros(len(x_train[0]))\n",
    "\n",
    "#run PLA and output in-sample error for test dataset. \n",
    "wp = PLA(w, x_train, y_train, 100)\n",
    "print(ErrorRate(wp, x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d48a3c6c215801e25502e9d265edcd29",
     "grade": false,
     "grade_id": "cell-beeae3997a377968",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Pocket algorithm [14 points]\n",
    "\n",
    "Implement a function for the pocket algorithm which accepts three arguments: the data, the labels, and the number of iterations it executes. It should use the function `PLA` you developed above. It returns the updated weight vector.\n",
    "\n",
    "First, briefly describe how your pocket algorithm works, and how it calls the function `PLA` above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6f36bbd3bd478262c9b7a824869b20c5",
     "grade": true,
     "grade_id": "cell-a8758f5f83e66912",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "As the recursive PLA training proceeds, the updated weight's in-sample error can increase if the next data point is far from or on the opposite side of the ideal descision boundary. The Pocket algorithm mitigates this problem by keeping the best weight vector during training. The best weight vector is that with the lowest error This solution comes at the cost of lower computational efficiency due to calculating the in-sample error each iteration. \n",
    "\n",
    "For each iteration in a loop, the pocket algorithm runs a single iteration PLA, followed by an evaluation of the in-sample error of the entire training set. When the in-sample error for the current set is the lowest on record, that set of weights is saved. The loop repeated a pre-selected number of times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9641cc289d555169a3b843903f1b5452",
     "grade": true,
     "grade_id": "cell-6db776985174dd90",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def pocket(x, y, T):\n",
    "    \n",
    "    #Create Weight Vector\n",
    "    w = np.zeros(len(x[0]))\n",
    "    \n",
    "    minErr = 1000000;  #arbitrarily selected large number for minErr, known to be larger than the maximum in-sample error\n",
    "    wPocket = PLA(w, x, y, 1);\n",
    "\n",
    "    #Run PLA T times and save the weight vector with the lowest error. \n",
    "    for i in range(T): \n",
    "        \n",
    "        #Run PLA to obtain next weight vector\n",
    "        w = PLA(w, x, y, 1)\n",
    "        \n",
    "        #Evalueate Ein \n",
    "        Ein = ErrorRate(w, x, y)\n",
    "\n",
    "        #Save the new weight vector if it produces a lower error than previous weight \n",
    "        if(Ein < minErr):\n",
    "            minErr = Ein\n",
    "            wPocket = np.copy(w)\n",
    "    return wPocket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a5f59bdc0a619eaf1c7fbecc1850c6ca",
     "grade": false,
     "grade_id": "cell-d4700c0f8f5efa9b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test the `pocket` function by training the classifier on the training data `(trainData, trainTarget)` with 100 iterations, and measuring the cassification error using the testing data `(testData, testTarget)`. Write the test script into the box below and let it print the classiciation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75dcbaf25c695d58723ac914b8f11732",
     "grade": true,
     "grade_id": "cell-87a6bd87fb24fb25",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.034482758620689655\n"
     ]
    }
   ],
   "source": [
    "trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n",
    "\n",
    "#flatten x matrix from 28x28 to 784x1 vector and add bias to each input set\n",
    "x_train = [k.flatten() for k in trainData]\n",
    "x_train = np.hstack( (np.vstack(np.ones(len(x_train))), x_train) )\n",
    "x_valid = [k.flatten() for k in validData]\n",
    "x_valid = np.hstack( (np.vstack(np.ones(len(x_valid))), x_valid) )\n",
    "x_test = [k.flatten() for k in testData]\n",
    "x_test = np.hstack( (np.vstack(np.ones(len(x_test))), x_test) )\n",
    "\n",
    "#re-scale y vector from 0->1 to -1->+1\n",
    "y_train = [-1+2*k[0] for k in trainTarget]\n",
    "y_valid = [-1+2*k[0] for k in validTarget]\n",
    "y_test = [-1+2*k[0] for k in testTarget]\n",
    "\n",
    "#run pocket algorithm and output in-sample error for test dataset. \n",
    "wp = pocket(x_train, y_train, 100)\n",
    "print(ErrorRate(wp, x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6a7ed57aa02cf3caa6db301fda7f05c",
     "grade": false,
     "grade_id": "cell-a22b06b71e7333b8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "State the test error results for the PLA and pocket algorithm that you obtained. Briefly discuss if they are as you expected and why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2045865b533306dadc6541971809cc72",
     "grade": true,
     "grade_id": "cell-ee0749d36e1f6ba7",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The PLA had a test error of 0.02759\n",
    "\n",
    "The pocket algorithm had a test error of 0.03448\n",
    "\n",
    "The PLA will train the entire set of input data and outputs the final updated weights. The pocket algorithm evaluates the error after each PLA update and saves weight file with the lowest error. \n",
    "\n",
    "As PLA proceeds to update weights, the error can go up and down as different training inputs will train the weights in different directions which are best for that training cycle, not the general dataset. The pocket algorithm overcomes this by only saving the set of weights which solve the training set with the lowest error. \n",
    "\n",
    "It was expected that the pocket algorithm would train the weights to better predict the test set. However it was observed that the PLA trained the weights better to predict the test set. By chance, the final updates in the PLA algorithm produced a weight set to better predict the test set than the 'pocketed' weights which best predicted the training set. \n",
    "\n",
    "In other words, the best set of weights trained in the pocket algorithm is best suited for the training set. When the weights were used to predict the target of the test set it produced a slightly higher error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a991507e56c1c7bb3a0f686df6e80515",
     "grade": false,
     "grade_id": "cell-e63e3e21f2a9ad08",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "The second classifier we consider is logistic regression.  Logistic regression computes the probability measure\n",
    "$$\\hat{y}=\\theta(\\underline{w}^T\\underline{x})$$\n",
    "for a feature vector $\\underline{x}$, where $\\theta(z)=\\mathrm{e}^s/(1+\\mathrm{e}^s)$ is the logistic function. Given $N$ data samples $\\underline{x}_n$ and labels $y_n$, the error measure for logistic regression is the binary cross-entropy loss\n",
    "$$L(\\underline{w})=\\frac{1}{N}\\sum\\limits_{n=1}^N\\left[-y_n\\log\\left(\\hat{y}(\\underline{x}_n)\\right)-(1-y_n)\\log\\left(1-\\hat{y}(\\underline{x}_n)\\right)\\right]\\;.$$\n",
    "For the following, you will consider the regularized loss function\n",
    "$$L_\\lambda(\\underline{w})=\\frac{1}{N}\\sum\\limits_{n=1}^N\\left[-y_n\\log\\left(\\hat{y}(\\underline{x}_n)\\right)-(1-y_n)\\log\\left(1-\\hat{y}(\\underline{x}_n)\\right)\\right]+\\frac{\\lambda}{2}\\|\\underline{w}\\|_2^2$$\n",
    "with the regularization parameter $\\lambda \\ge 0$. \n",
    "\n",
    "The training of the weight vector $\\underline{w}$ will be performed through batch gradient descent. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "db5e733b839e11cc95526c870678e2a1",
     "grade": false,
     "grade_id": "cell-68358d532c780cc5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Loss function [3 points]\n",
    "\n",
    "Implement a function to compute the regularized cross-entropy loss as defined above. The function has four input arguments: the weight vector, the feature vectors, the labels, and the regularization parameter. It returns the regularized loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "364ced88d6193816c529eb80c8df2b34",
     "grade": true,
     "grade_id": "cell-aebbe742cfbb66f8",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return (1/(1+np.exp(-x)))\n",
    "\n",
    "def crossEntropyLoss(w, x, y, reg):\n",
    "    L = 0;\n",
    "    N = len(x)\n",
    "    for i in range(N):\n",
    "        y_hat = sigmoid(np.dot(w,x[i])).clip(10**(-10), 1 - 10**(-10))\n",
    "        L += ( -1*y[i]*np.log(y_hat) - (1 - y[i])*np.log(1 - y_hat) )\n",
    "    L /= N     \n",
    "    L = L + (reg/2*np.power(np.linalg.norm(w), 2))/len(y)\n",
    "                        \n",
    "    return L   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e75d612d7fa35907b7240385d95df34f",
     "grade": false,
     "grade_id": "cell-8d0a18f2be2713da",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Gradient [4 points]\n",
    "\n",
    "Provide an analytical expression for the gradient of the regularized cross-entropy loss with respect to the weight vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4d7b1b6eb20f9afc7eff02678dc1ec9",
     "grade": true,
     "grade_id": "cell-bea8165ab7196506",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "$$\n",
    "g_t = \\nabla L_ \\lambda (\\underline w) = \\frac{1}{N} \\sum_{n=1}^{N}\\biggl[-y_n \\frac{\\hat y'(\\underline x_n)} {\\hat y(\\underline x_n)} - (1-y_n) \\frac{-\\hat y'(\\underline x_n)}{(1- \\hat y(\\underline x_n))} \\biggr] + \\lambda \\underline w \\\\\n",
    "$$\n",
    "$$\n",
    "\\tag{1}\\label{1} 1g_t = \\frac{1}{N} \\sum_{n=1}^{N}\\biggl[ \\frac{ \\hat y'(\\underline x_n)(\\hat y (\\underline x_n)-y_n)}{\\hat y(\\underline x_n) ( 1 - \\hat y(\\underline x_n) )} \\biggr] + \\lambda \\underline w \\\\ \n",
    "$$\n",
    "$$\n",
    "where \\;\\;\\; \\hat y'(\\underline x_n)= \\frac{d}{dw} \\theta(\\underline w^T \\underline x_n) = \\frac{\\underline x_n e^{-\\underline w^T \\underline x_n}}{(1 + e^{-\\underline w^T \\underline x_n})^2}  \\\\\n",
    "\\\n",
    "which \\ simplifies \\ to \\;\\;\\;  \\hat y'(\\underline x_n)= \\underline x_n \\theta(\\underline w^T \\underline x_n) ( 1 - \\theta(\\underline w^T \\underline x_n)) \\\\\n",
    "$$\n",
    "$$\n",
    "\\tag{2}\\label{2} or \\;\\;\\; y'(\\underline x_n)= \\underline{x_n} \\hat{y}(\\underline x_n) (1 - \\hat{y}(\\underline x_n)) \n",
    "$$\n",
    "$$\n",
    "\\tag{3}\\label{3} Combining \\ \\ref{1} \\  and \\ \\ref{2}: \\;\\;\\; g_t = \\frac{1}{N} \\sum_{n=1}^{N} \\biggl[ \\underline x_n (\\hat y (\\underline x_n)-y_n) \\biggr] + \\lambda \\underline w \\\\ \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "20569096badf58c8909bc82a5df77f5b",
     "grade": false,
     "grade_id": "cell-8f0c5afca5e9d417",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Implement a function to compute the gradient. The function has four input arguments: the weight vector, the feature vectors, the labels, and the regularization parameter. It returns the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f8488b92818298680af5f1328ae5f347",
     "grade": true,
     "grade_id": "cell-b39c8177911aa256",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gradCE(w, x, y, reg):\n",
    "    \n",
    "    N = len(x)\n",
    "    sum_vector = np.zeros(len(x[0]))\n",
    "    \n",
    "    for i in range(N):\n",
    "        sum_vector += x[i] * (sigmoid(np.dot(w,x[i])) - y[i]);\n",
    "        \n",
    "    return sum_vector/N + reg*w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f73e7673edffccd37ae61138a708241",
     "grade": false,
     "grade_id": "cell-e62e6251dc4d5a69",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Gradient Descent Implementation [5 points]\n",
    "Using the gradient and cross-entropy loss function above, implement the batch gradient descent algorithm. The function should accept seven arguments: the weight vector, the feature vectors, the labels, the learning rate, the number of epochs, the regularization parameter, and an error tolerance (set to $10^{-7}$ for the experiments). The error tolerance will be used to terminate the gradient descent early, if the difference (i.e., its 2-norm) between the old and updated weights after one iteration is below the error tolerance. The function should return the optimized weight vector and the learning error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b2882a29db4cb0850585ece871680d6",
     "grade": true,
     "grade_id": "cell-df585dbae385a61c",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def grad_descent(w, x, y, eta, iterations, reg, error_tol):\n",
    "    \n",
    "    error = np.zeros(iterations)\n",
    "    w1 = np.copy(w);\n",
    "    g = gradCE(w1, x, y, reg)\n",
    "    v = -1.0*g / np.linalg.norm(g)\n",
    "    w = w + eta*v\n",
    "    error[0] = crossEntropyLoss(w1, x, y, reg)\n",
    "        \n",
    "    for i in range(1, iterations):\n",
    "        \n",
    "        #calculate opposite direction of gradient\n",
    "        g = gradCE(w1, x, y, reg)\n",
    "        v = -1.0*g / np.linalg.norm(g)\n",
    "        \n",
    "        #save previous weights for error calculations\n",
    "        w_prev = w1\n",
    "        \n",
    "        #update weights\n",
    "        w1 = w1 + eta*v\n",
    "        \n",
    "        #calculate cross entropy loss and save in list\n",
    "        error[i] = crossEntropyLoss(w1, x, y, reg)\n",
    "        \n",
    "        #calulate norm of difference between previous and current weights; exit if error lwee than error_tol\n",
    "        err_dif = np.linalg.norm(w_prev - w1);\n",
    "        if(err_dif < error_tol): \n",
    "            return w1, error   \n",
    "    \n",
    "    return w1, error  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "210fb89fe9e58d3cc7ae4d8767f0a92f",
     "grade": false,
     "grade_id": "cell-d446b8d2bb035471",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Tuning the Learning Rate [6 Points]: \n",
    "Write a script that excutes logistic regression using the gradient descent function from above to classify the two classes in the notMNIST dataset, loaded with the  `loadData` function. \n",
    "\n",
    "Set the number of epochs to $5,000$ and use the regularization parameter $\\lambda=0$. \n",
    "\n",
    "For the learning rate, consider $\\eta=5\\cdot 10^{-3},\\,10^{-3},\\,10^{-4}$.\n",
    "\n",
    "Train the classifier on the training data (`trainData, trainTarget`).\n",
    "\n",
    "The script should plot the training loss as a function of the number of training epochs and output the test loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ad70c3f83e48152234d667aaec5df51",
     "grade": true,
     "grade_id": "cell-776e56a74f0c1ff1",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for $\\eta$ = 0.005:  [0.15029116]\n",
      "Test Loss for $\\eta$ = 0.001:  [0.08986244]\n",
      "Test Loss for $\\eta$ = 0.0001:  [0.14680297]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEhCAYAAABycqfJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deXxU5b3H8c9vtux7whZIwr5KWBVBEUEQxQVF63q1VWutdbcq6tXWenuv3mJrVZTi2l6tuIKoiFIRFcUFECi7YZOwJmwBss88948zCUPIMpCcTCbze79e8zpnzpw58zsO5jvPec45jxhjUEopFbkcoS5AKaVUaGkQKKVUhNMgUEqpCKdBoJRSEU6DQCmlIpwGgVJKRThXqAs4XkuWLGnjcrleAPqhQaaUUg3xASsrKytvGDx48O7aVgi7IHC5XC+0a9eud0ZGxj6Hw6EXQSilVD18Pp8UFBT02blz5wvABbWtE46/qPtlZGQUaQgopVTDHA6HycjIOIB1FKX2dZqxnqbi0BBQSqng+f9m1vn3PhyDQCmlVBPSIFBKqQinQdCCvP3224k5OTn9srKy+j3wwAPtjmed+t6bmZl5Uo8ePfr06tWrT79+/XrbvR+qbnZ9x5deemlOampqbvfu3fvavQ+qfnZ9x3W91hTfvQZBC1FZWcmdd96ZNWfOnPXr169f9c4776QuWbIkOph1gnnv559/vn7t2rWrV65cuaZ590xVsfM7vu666wpnz579Y/PvlQpk13dc32tN8d1rEJygBx98sN0VV1yRfdZZZ3XNzMw86eGHH27bmO0tWLAgLjs7u6xPnz7l0dHR5uKLL9779ttvJwezTjDvVccvnL7jc84551BGRkZlY+qLROHyHdf3WlN89xoEJ2jlypUxZWVlMnfu3A1z585dP2PGjLSa6wwePLhnr169+tR8zJo1K6Hmulu3bvVkZmaWVz3v2LFj+bZt2zzBrBPMe8eMGdO9b9++vadMmZLe2H2PFOH2HavjFy7fsd3ff9hdUBbonreXd1q/82BsU26zR7uE4j9dkru1ofXWrFkTM3PmzA0ulwun02mSk5O9NddZsmTJumA/t7YBgkTEBLNOQ+/96quv1ubk5FRs27bNNXr06B59+/YtPeeccw4FW1tIzfpNJ3avbtLvmDZ9ipk4tVV9x+Hsoa8e6pS3L69Jv+NuKd2KHx3xaKv5ju3+/sM6CEKlrKxM9u/f7+rbt28ZwJIlS2J79+5dXHO9wYMH9zx8+LCz5vLHHnts68SJEw8GLsvKyjoq4fPz8z0dOnSoCGadht6bk5NTAZCZmVk5YcKE/YsWLYoLmyAIkXD7jtXxC6fv2O7vP6yDIJhf7nZYsWJFVNeuXUurnv/www+xubm5JTXXO55fEmecccbhzZs3R69du9aTk5NT8e6776a+9tprG4NZJzc3t7Su9xYVFTm8Xi8pKSm+oqIix2effZb44IMPbm/M/jerIH652yGcvuNwF8wvdzuE03cczHYbQ/sITsDSpUtj+/TpU/0PZsWKFTGDBw8+5pfE8XC73TzxxBM/jR8/vkf37t37Tpw4ce+QIUNKAc4444xumzdvdte1Tn3vzc/Pdw0bNqxXz549+wwaNKj3uHHj9l9yySVFjfsv0PqF03cMcP7553c+7bTTem3atCmqbdu2/f/yl79oX1ADwuk7ru+1pvjuJdwGr1++fPnm3NzcwlDXESgrK6vf6tWrV8XHx4fXf0wVNP2OW7/W/h0vX748PTc3N6e217RF0EiFhYVOt9ttWus/HqXfcSSI9O9Yg6CR0tPTvRs2bFgV6jqUffQ7bv0i/TvWIFBKqQinQaCUUhFOg0AppSJcOAaBz+fzSaiLUEqpcOH/m+mr6/VwDIKVBQUFSRoGSinVMP+YxUnAyrrWCbsriysrK2/YuXPnCzt37uxHeAaZUko1Jx+wsrKy8oa6Vgi7C8qUUko1Lf1FrZRSEU6DQCmlIlzY9RGkp6ebnJycUJehlFJhZcmSJYXGmIzaXgu7IMjJyWHx4sWhLkMppcKKiGyp6zU9NKSUUhFOg0AppSKcrUEgIuNFZJ2I5InI5Fpev0dElvkfK0XEKyKpdtaklFLqaLYFgYg4ganAOUAf4AoR6RO4jjHmT8aYAcaYAcD9wOfGmL121aSUUupYdrYITgbyjDEbjTHlwAzgwnrWvwJ43cZ6lFJK1cLOIMgEAgelzvcvO4aIxALjgXdsrEcppVQt7AyC2m4KV9f9LM4HvqrrsJCI3Cgii0VkcUFBQZMVqJRSyt4gyAc6BTzvCGyvY93LqeewkDFmujFmiDFmSEZGrddDNKiwpJDHv3ucCm/FCb1fKaVaKzuD4Hugu4h0FhEP1h/72TVXEpEk4AzgPRtrYemyV3h1zav895cPoDfaU0qpI2wLAmNMJXAL8DGwBnjTGLNKRG4SkZsCVr0I+MQYc9iuWgDGpfXn+v0HeHvLXN5Y94adH6WUUmHF1ltMGGPmAHNqLJtW4/krwCt21gFAXDq37jvAjzmn8Ph3j9M1uStD2w21/WOVUqqli5gri7/d7cAJ3Jc0iqzELO5acBf5B/NDXZZSSoVcxATBfhIBiC4+wFOjn8JrvNz22W0UVxSHuDKllAqtiAkCR1Qch0w0juJCshOzmTJyChv2b+CBhQ/gM3WO6ayUUq1exASByynsMYk4igsBGJ45nLsH382nP33KtOXTGni3Ukq1XhETBG6Hgz0k4iwprF72H33+gwu6XsBzy59j3pZ5IaxOKaVCJ2KCwOmwWgSu0iMXL4sID5/6MP0z+vPgwgdZu3dtCCtUSqnQiJggcDurgmDPUcujnFE8OepJEjwJ3PLpLewu3h2iCpVSKjQiJghcTuvQkLt0L/iO7hzOiM1g6pipFJUXcev8W/VMIqVURImcIHAIe0wSDlMJpfuPeb1Xai/+NPJPrN27lvu/vF/PJFJKRYzICQKnsMckWE8OF9a6zhmdzuCeIfcwf+t8nlzyZDNWp5RSoWPrLSZaEpfDwR6SrCfFhUCPWte7qvdVbC7azMurXiYrMYtLelzSfEUqpVQIREwQVHUWA3C47jENRITJJ08m/1A+f/zmj2TGZ3Jqh1ObqUqllGp+EXRoyEFhEEEA4HK4mDJyCjlJOdy94G427t/YDBUqpVRoRE4QOIR91N9HECjeE8/UMVNxO93c/OnN7C2tdfA0pZQKexEVBJW4KHMlNtgiqNIhvgNPj36awpJCbv30VkoqS2yuUimlml/kBIHT2tUST2pQLYIq/TP68/jIx1m5ZyX3fn4vlb5Ku0pUSqmQiJggcDsFgBJ3ynEFAcCYrDHcf/L9LMhfwB+//aMOdamUalUi5qwhp8MKgsPuVDh8/APSXN7rcnYe3smLK1+kXWw7fpX7q6YuUSmlQiJigsDtsBo/h13JcOCHE9rG7YNuZ3fxbp5Z9gxt49oysdvEpixRKaVCImKCwOEQHALFrhQo2QveSnAe3+6LCI8Mf4SCkgJ+//XvSY9J57TM02yqWCmlmoetfQQiMl5E1olInohMrmOdUSKyTERWicjndtbjcjo46Ey2npSc2Omgbqebv4z6C91TunPXgrtYtWdVE1aolFLNz7YgEBEnMBU4B+gDXCEifWqskww8C1xgjOkLXGpXPQBuhxwJgkMnfrvpeE88z455lpSoFH7zr9+w9eDWJqpQKaWan50tgpOBPGPMRmNMOTADuLDGOlcC7xpjfgIwxtg6GIDTIRxwpVpPDu1q1LYyYjN4buxzVJpKfjXvVxSWHN+ZSEop1VLYGQSZQOBP5Xz/skA9gBQRWSAiS0TkGhvrwe10sN9ZFQSNz5wuSV2YOmYqhSWF/GrerygqL2r0NpVSqrnZGQRSy7KaJ+C7gMHABOBs4CEROea2oCJyo4gsFpHFBQXBXRVcG5dT2C8p1pNDO094O4FyM3J58swn2XhgI7d8eotefayUCjt2BkE+0CngeUdgey3rzDXGHDbGFAJfALk1N2SMmW6MGWKMGZKRkXHCBbkcDg4TA574JmkRVBneYTiPn/44ywuWc9eCu6jwVjTZtpVSym52BsH3QHcR6SwiHuByYHaNdd4DThcRl4jEAqcAa+wqyOUUKn0+iG8DB5umRVBlXM44Hhr2EAu3LeTBhQ/i9XmbdPtKKWUX264jMMZUisgtwMeAE3jJGLNKRG7yvz7NGLNGROYCKwAf8IIxZqVdNbkcQqXXQHy7RncW1+aSHpdQVF7EX5b8hcSoRB485UFEajtCppRSLYetF5QZY+YAc2osm1bj+Z+AP9lZRxW303GkRbDLnry5rt917C/bz8srXyYpKolbB95qy+copVRTiZgri8F/aMhrIKEd5H1q2+fcOehOisqKmL5iOomeRK7te61tn6WUUo0VUUHgdDio8BmIbwvlB6H8MHjimvxzRISHhj3EwfKDTFk8BY/TwxW9rmjyz1FKqaYQUUHgdgiVXp8VBGD1E6R2seWznA4nj418jIoFFfz3t/+Nx+FhUo9JtnyWUko1RsSMRwBVZw0ZSKgKAlsvZMbtcDPljCmMyBzBI4se4f0N79v6eUopdSIiKgjcTsfRLYImPoW0Nh6nhydHPcnQdkP5z6/+k483f2z7Zyql1PGIqCBwOvwtgvh21gKbWwRVol3RPD36aQZkDGDyF5OZ/9P8ZvlcpZQKRkQFgcvhoMJrIDYNxNlkt5kIRqw7lqljptI7rTd3f343X+Z/2WyfrZRS9YmoIHA7/Z3FDod1LYENF5XVJ94Tz3NnPUf35O7c8dkdLNq+qFk/XymlahNRQeByOvD6/Pe9i28DB5s3CACSopL429i/kZ2Uza3zb+XrbV83ew1KKRUosoLAIVT4fNYTm24zEYyU6BReHPci2YlWGCzctjAkdSilFERgEFR6/S2ChLYhCwI4EgZdkrtw2/zb+CL/i5DVopSKbJEVBE5/ZzFYp5AeLoAQ3iU0OTqZF8a9QPeU7tz+2e0s2LogZLUopSJXRAWBu+o21GAFgfFZYRBCSVFJPD/ueXql9OLOBXfy6U/23QNJKaVqE1FB4HI48FYfGmpvTQ/uCF1BfomeRKaPm06ftD78dsFvmbdlXqhLUkpFkMgKAmdAZ3FiB2taVHPQtNBI8CTwt7P+Rr/0ftzz+T3M3TQ31CUppSJEZAVBYGdxYqY1bSFBANZ1BtPGTmNAmwHc+8W9vLP+nVCXpJSKAJEVBE4HlT6DMQbiMsDhalFBABDnjuO5s55jeOZwfr/o9/x91d9DXZJSqpWLqCBwO6xhI70+Y11dnNChxQUBQIwrhqfPfJpx2eOYsngKU5dNtcJLKaVsEFHjETidVhBU+gwuJ1Y/QdG20BZVB7fTzf+O/F9iF8Uybfk0DpUf4p6h9+CQiMpupVQziKggcDusP6IVXh/RbqcVBDuWh7iqujkdTh4Z/gjx7nheXfMqhysO87tTf4fT4Qx1aUqpVsTWn5ciMl5E1olInohMruX1USJyQESW+R8P21mPq6pFUN1h7D801IIPuzjEwb1D7+Wm3JuYmTeTe7+4lwpvRajLUkq1Ira1CETECUwFxgL5wPciMtsYs7rGql8aY86zq45Abqe/RRB4CmllCZTsg9jU5ijhhIgIvxnwG+Ld8UxZPIVDFYf486g/E+du+vGWlVKRx84WwclAnjFmozGmHJgBXGjj5zXI4w+C8soa1xK0gIvKgnFt32v5w/A/8O2Ob7nu4+soLCkMdUlKqVbAziDIBLYGPM/3L6vpVBFZLiIfiUhfG+vB7bIODVW04GsJGnJR94t4avRTbNy/kWs+uoatRVsbfpNSStXDziCQWpbVPBi/FMg2xuQCTwOzat2QyI0islhEFhcUnPi9gTxOq5O1wlvz6uKWeeZQXUZ2HMkLZ7/AwfKDXP3R1azasyrUJSmlwpidQZAPdAp43hE46qe3MabIGHPIPz8HcItIes0NGWOmG2OGGGOGZGRknHBBbn9ncfWhofi2II6wahFUyc3I5R/n/INoZzTXzb2Or7frADdKqRNjZxB8D3QXkc4i4gEuB2YHriAi7URE/PMn++vZY1dBHpe/j6CqReB0W2EQZi2CKp2TOvN/5/4fHRM68pt//YYPNn4Q6pKUUmHItiAwxlQCtwAfA2uAN40xq0TkJhG5yb/aJcBKEVkOPAVcbmy8hPaYzmI4cgppmGoT24ZXxr/CwLYDuf/L+3nx3y/qVchKqeNi6wVl/sM9c2osmxYw/wzwjJ01BHK7jlxQVi2xAxT+2Fwl2CLBk8C0s6bx4MIHeXLpk2w9uJUHhz2I2+EOdWlKqTAQUfcrqGoRHBUECR2gKDxOH62Px+nh8ZGP88uTfsk7P77Dzf+6maLyolCXpZQKAxEVBO7aDg0lZULZASgN/z+aDnFw26Db+MPwP7B452KumXMN+QfzQ12WUqqFi6ggONJZHHAMPTnLmh5oPefjX9T9Iv429m/sLtnNVXOuYkXBilCXpJRqwSIrCGptEfiDYP9PIajIPie3P5lXz32VWFcs1318HZ9s/iTUJSmlWqiICoIjVxYHBEFy6wwCgC5JXXhtwmv0Tu3N3Z/fzfMrntczipRSx4ioIKi1szguHVwxrTIIAFKjU3nh7Bc4p/M5PPXDU9z7xb2UVJaEuiylVAvSYBCIyMUikuCfnywib4rIAPtLa3pVp48edWhIBJI7tdogAIhyRvH46Y9zx6A7+Hjzx1z70bXsOBT+Z0oppZpGMC2C3xtjDorIcOB84A1gWgPvaZGq+wgCWwRgHR5qxUEA1q2srz/pep4Z8wxbD27l8g8vZ8muJaEuSynVAgQTBF7/9DzgWWPMO0CUfSXZp9bTR8EKglZ01lB9RnYcyWsTXiPRk8gNn9zAW+vfCnVJSqkQCyYIdojIVOAyYI7/vkFh2bfgdAhOhxzdRwCQ1AmK90DZodAU1syqOpFPaX8Kf1j0B/7rm/+iwqejnikVqYL5g/4z4HNggjFmH5AOHDPsZLjwOB1HxiOo0gqvJWhIoieRqaOn8ou+v+CNdW9ww8c3UFB84rf4VkqFr2CCIB14zxizVkROAyYCX9lbln3cTqnl0FC2Nd0fOUEA4HQ4uWvIXTx2+mOs2buGn33wMxbvXBzqspRSzSyYIJgF+ESkK/APoDfwT1urspHH5ayls9g/bML+Lc1fUAswocsEXj33VeLccdzwyQ38fdXf9XoDpSJIMEHgM8ZUABcDTxpjbqX2ISfDgqe2FkFcG3BGtfozh+rTI6UHMybM4MxOZzJl8RTu/vxuDpVHRp+JUpEumCCoFJFLgf8AqkY+Cdv7G7tdjmM7ix0OSOoYUX0EtYn3xPPnUX/m7sF3M/+n+Vzx4RXk7csLdVlKKZsFEwTXAWcC/2uM2SginYHX7S3LPlZnse/YFyLgWoJgiAg/7/dznh/3PAfLD3LlnCt15DOlWrkGg8AYsxK4DVgsIr2ArcaYP9pemU3cTsexh4YAUnJg3+bmLqfFGtpuKG+d/xa9U3tz/5f38/BXD1NcURzqspRSNgjmFhOnA3nAi8BLwHoRGWF3YXZxuxxH34a6SmoX61qCkv3NX1QLlRGbwYtnv8gvT/ols/JmccWHV7B+3/pQl6WUamLBHBr6C3CuMWaEMWY4MAH4q71l2SfK6aC80nvsC6ldrOm+Tc1bUAvncri4bdBtTB83naLyIq788EreXPemnlWkVCsSTBB4jDGrq54YY9YAnmA2LiLjRWSdiOSJSJ0XoYnIUBHxisglwWy3MdwuOfaCMjgSBHs32l1CWBrWfhhvnf8WQ9oO4dFvHuXuz+/WoTCVaiWCCYKlIvI3ETnN/3gO+KGhN4mIE5gKnAP0Aa4QkT51rPc48PHxlX5i6uwsTsmxphoEdUqPSefZs57lrsF38dlPn3Hp7EtZXrA81GUppRopmCC4CdgA3AvcB2wEbgzifScDecaYjcaYcmAGcGEt690KvAPsDqriRqqzs9gTaw1kv1cPDdXHIQ5+0e8XvHLOK4gI1350Lc8te45KX2WoS1NKnaBgzhoqNcb8rzHmAmPM+caYP2F1GjckEwg8MT+fGheiiUgmcBHNeFtrq7O4liAA6/CQtgiCkpuRy5vnv8n4zuN5dvmzXPvRtWwpiswrs5UKdyd6F9HTg1hHallW8+D8k8B9xphaem8DNiRyo4gsFpHFBQWNuzFaVF0tAoDUzrBnQ6O2H0kSPYk8dvpj/Gnkn9hctJlL379UO5KVCkN23k46H+gU8LwjsL3GOkOAGSKyGbgEeFZEJtbckDFmujFmiDFmSEZGRqOKctfVRwBWi+Dwbig72KjPiDTjO4/n3QveJTcjl0e/eZRb599KYUlhqMtSSgWpziAQkf51PHIJ7hYT3wPdRaSzfwyDy4HZgSsYYzobY3KMMTnA28DNxphZJ747DfO4arkNdZXqM4e0n+B4tY1ry9/G/o37ht7Hou2LuPi9i5n/0/xQl6WUCoKrntem1vNagzegMcZUisgtWGcDOYGXjDGrROQm/+shGe7S43JQVlHHkajAU0jb92++oloJhzi4us/VnNrhVCZ/OZnbP7ud87ucz30n30dSVFKoy1NK1aHOIDDGBNMPUC9jzBxgTo1ltQaAMebnjf28YES7HZTW10cA2mHcSF2Tu/LPc//JtBXTePHfL7JoxyIeGvYQo7NGh7o0pVQtwnLIycaIcjnx+gyVtfUTRCVYt6TWDuNGczvd3DrwVl6f8Dpp0Wnc/tnt3PvFvewr3Rfq0pRSNURcEES7rV2us1WQ3gMK9X46TaV3Wm9en/A6Nw+4mXmb5zHxvYl8svmTUJellAoQgUHgBKC0rn6CjB5QuA70FMgm43a6+XXur5lx3gzaxrbl7s/v5q4Fd7GnZE+oS1NKEdzdR2s7cyhbRMIyRKJcVtlldbYIekLpATjULBc6R5SeqT15bcJr3DbwNhZsXcAFsy7g3R/fxWfq+C6UUs0imD/mLwJLsMYr/j9gMTAT+FFExthYmy2CahGA1SpQTc7tcPPL/r/k7fPfpltyN3739e/4xdxfsGG/9ssoFSrBBMGPwGBjzABjTC4wGFgGnA08YWdxdohyNRAE6T2taYEGgZ26JHfh5fEv88jwR8jbn8cl71/C0z88TWllaahLUyriBBMEvY0xK6qeGGP+DQwyxoTlYLZR7gYODSV2AE+Cdhg3A4c4uLj7xcyeOJvxOeOZvmI6k2ZPYtH2RaEuTamIEkwQbBCRp0VkhP/xFJAnIlFA2N1yMrqhFoGIdXhIWwTNJi0mjf85/X+YPnY6ADfOu5HJX07W21Qo1UyCCYJrsO4bNBm4H+t+QddihUAY9hH4WwQV9XRQpvfUFkEInNrhVN654B1+edIv+Xjzx5w38zz+vurvVPgqQl2aUq1aMLehLjbGPO6/BfV5xpjHjDGHjTFeY8yB5iiyKVX1EZTVNlxllYwecHCHdfaQalbRrmhuG3QbMy+YycA2A5myeAqTZk/i6+1fh7o0pVqtYE4fHSYiH4nIahFZX/VojuLsUH1BWUMtAoCCsN3NsJeTlMOzY57l6dFPU+Gt4FfzfsWdn93J9kM1b2CrlGqsYA4NvQw8C5yFNQ5B1SMsNXj6KEBGVRCsbYaKVF1EhFGdRjFr4ixuHXgrC7ct5IJZF/Dc8uf07CKlmlAwQVBkjHnfGLPdGLOr6mF7ZTZp8IIysMYvdsfCrlXNU5SqV5Qzihv738jsibM5o+MZPLvsWS6YdQEfbvxQL0ZTqgkEEwTzReR/RGRo4NXFtldmk6BaBA4ntOkDu1Y2U1UqGO3j2/PEqCd4cdyLJEclM/nLyVz14VUs2bUk1KUpFdbqG4+gymk1pmANOTmy6cuxX1WLoN4+AoB2/WDVLOueQ1LbqJsqVE5ufzIzzpvBBxs/4K9L/8rP5/6cMVljuHPwnWQnZoe6PKXCToNB0BTjErQkLqcDl0More+sIYC2/WDJK1C0DZI6NkttKngOcXBB1wsYmz2Wf6z6By+ufJHPt37OZb0u46b+N5EcnRzqEpUKG3UGgYhcYYx5XURuq+11Y8xT9pVlr2i3s/7rCADanWRNd67UIGjBYlwx/Cr3V0zqMYmpy6by+trXmZ03m+tOuo6rel9FjCsm1CUq1eLV10eQ4p9m1PEIW9YoZQ21CPpa013/tr8g1WjpMen87tTf8c757zCgzQD+uvSvnPvuucxYO4MKr16QplR96huq8ln/9KHmK6d5RLmc9XcWgzVaWUqO1SJQYaNbSjeePetZlu5ayl+X/pU/fvtHXln1Cr8Z8BvO7XwuTocz1CUq1eIEc0FZuojcKyLPisj0qkdzFGeXKLej/tNHq7Ttp2cOhalBbQfxyvhXeO6s50j0JPLAwgeYNHsSn275FKODDil1lGBOH30PaAssBD4NeDRIRMaLyDoRyRORybW8fqGIrBCRZSKyWEROq207TS3a5aSsoRYBWP0EezZA+WH7i1JNTkQ4LfM0Zpw3gyfOeAKv8XLHgju48sMr+TL/Sw0EpfyCOX00zhhz9/FuWEScwFRgLNZN674XkdnGmNUBq30KzDbGGP+1CW8CvY73s45XtNvR8OmjAO36A8Y6PJR1it1lKZs4xMG4nHGMzhrN+xveZ9ryadz86c30S+vHTbk3MbLjSERPEVYRLJgWwUciMu4Etn0ykGeM2WiMKQdmABcGrmCMOWSO/CyLw7o+wXaxHhfF5UHcQTtzkDXdphcstQYuh4uLul/EBxd9wO9P/T37yvZxy/xbuOyDy5j/03xtIaiIFUwQ3ATMFZFDIrJXRPaJyN4g3pcJbA14nu9fdhQRuUhE1gIfAtcFU3RjxXqcFJcHcWgooR0kdtQgaGXcTjeTekzi/Yve59ERj3Ko4hC3f3Y7l75/KfO2zNPbVqiIE0wQpANuIAnrtNF0gjt9tLa29jE/uYwxM40xvYCJwKO1bkjkRn8fwuKCgoIgPrp+cVEuDgfTIgCrVaBB0Cq5HW4mdpvI7Imz+e/T/psybxl3LbiLSbMn8eHGD6n0hd24S0qdkDqDQES6+2f71vFoSD7QKeB5R6xBbWpljPkC6Coi6bW8Nt0YM8QYMyQjo/GXMMR6nBSXBdEiAMgcDPs2QfRk4zoAACAASURBVHEwjSAVjlwOF+d3PZ9ZF87i8dMfxxjD5C8nM+HdCby25jWKK4pDXaJStqqvs3gycD1Wh29Nwdxr6Hugu4h0BrYBlwNXBq4gIt2ADf7O4kGAB9gTZO0nLOhDQ3Ckn2D7Uuh2ln1FqZBzOpyc2+Vcxncezxf5X/DSypd47LvHmLZ8Glf2upLLe11OSnRKwxtSKszUd0HZ9f7pCd1ryBhTKSK3AB8DTuAlY8wqEbnJ//o0YBJwjYhUACXAZaYZeuxiPS5KKrx4fQano4GzRdoPAAS2aRBECoc4GNVpFKM6jeKH3T/w0sqXeHb5s7y86mUu6nYR1/S9hsz4Y7q7lApbwZw+ioj0AvoA0VXLjDH/bOh9xpg5wJway6YFzD8OPB5ssU0lLsq6urSkwkt8VAP/CaITrYFqtJ8gIg1sM5CnRz/Nhv0beGXVK7y5/k3eWPcG47LHcVWfq8jNyA11iUo1WjBXFv8nMB2YBpwDPAlcYnNdtor1WH/8gzqFFKx+gvzvrVtSq4jUNbkrj454lI8u/oire1/Nwm0LuXrO1Vz54ZXM2TiHCp/ez0iFr2DOGroMOBPYYYz5DyCXIFsSLVWsx2oRBN1hnDUMivdA4Y82VqXCQbu4dvx26G/516X/4oFTHuBg+UHu+/I+xr89nukrprO3VE8qUOEnmCAoMcZ4gUoRSQB2Al3sLcteVS2CoE8hzRpuTX/62qaKVLiJdcdyRa8reG/ie0wdM5VuKd14+oenGfvWWB7+6mHW7V0X6hKVClowv+x/EJFk4CVgMVAELLW1KptV9xEEe+ZQWleIawNbvobBP7evMBV2HOJgZMeRjOw4kg37N/Damtd4f8P7zMybycA2A7m0x6WMyxlHlDMq1KUqVad6WwRi3YDl98aY/caYqcAE4FfGmGuapTqbHGkRBBkEIpA93AoCperQNbkrD5/6MP+69F/8dshv2VOyhwcWPsBZb53FE4ufYEvRllCXqFSt6g0C/6mcHwQ8zzPGhHVrAAL7CI7jytHs4XBgK+z/yaaqVGuRFJXEtX2v5f2L3uf5cc8ztN1QXl39KufNPI9ffvJL5m2Zp53LqkUJ5tDQdyIyqDUEQJW4420RgBUEAFsWQXKWDVWp1sYhDoa1H8aw9sMoKC5gZt5M3l7/NnctuIv0mHQu6nYRE7tNJCtR/z2p0KrvFhNVIXEaVhisE5GlIvKDiIR1KMRW9xEcR4ugTR+ISoItC22qSrVmGbEZ3Nj/Rj66+COmjplKn7Q+vLjyRSbMnMC1H13LzB9n6q0sVMjU1yL4DhiEdTO4VuWEWgQOJ+ScBhsXWNcT6P3r1QlwOpzVncu7Du/i/Y3vMytvFg9//TD/893/cHbO2UzsNpFBbQbpGAmq2dQXBAJgjNnQTLU0m2i3AxE4fDx9BADdRsO6D61Ry9K72VOcihht49pyw0k3cH2/61lWsIxZebOYu2kus/JmkZ2YzYVdL+T8rufTLq5dqEtVrVx9QZAhInfV9aIx5s821NMsRIT4KBcHS48zCLqOtqYb5msQqCYjIgxsM5CBbQZy39D7mLdlHrPyZvHUD0/x9A9PM6TdEM7tfC5js8eSFJUU6nJVK1TfWUNOIB5IqOMR1hKj3RSVHueZG6ldIKUzbAhqyGaljlusO5YLu13Iy+NfZs5Fc/j1gF9TUFzAI4se4cw3z+T2+bfz8eaPKa0sDXWpqhWpr0Wwwxjzh2arpJklxriPv0UA0G0MLHsdKsvB5Wn6wpTy65TYiV/n/pqb+t/E6r2r+XDjh8zdNJf5W+cT545jTNYYJnSZwCntTsHpcIa6XBXGGuwjaK0So10UlZzAudxdR8P3L8DWb6HzCd2hW6njIiL0TetL37S+3D34br7f9T0fbvyQf235F7M3zCYtOo2zss9iXPY4BrUdhMsR1rcCUyFQ37+YMc1WRQgkxrjJ31dy/G/sPBIcbvjxYw0C1eycDmf1tQkPnvIgX277ko82fcR7ee/xxro3SI1OZUzWGMZmj2Vou6EaCioo9Q1M06pvo5hwoi2CqATocgas+QDGPqqnkaqQiXZFMzZ7LGOzx1JcUczCbQuZt2UeH2z8gLfWv0VyVHJ1KJzc/mTcDneoS1YtVMT+XDihzuIqvc6DD+6A3auhbTDDNytlr1h3LONyxjEuZxwllSV8ve1rPtnyCXM3z+WdH98h0ZPIqE6jOLPTmQzvMJxYd2yoS1YtSOQGQYybQ2WV+HwGR0PDVdbU81z44E6rVaBBoFqYGFcMY7LHMCZ7DGXeMr7e9jXztsxjwdYFzN4wG4/DwyntT6kejrNNbJtQl6xCLHKDINqFMXCwrJKkmONsMie0hU4nw9r3YdR99hSoVBOIckZxZtaZnJl1JhW+CpbtXsZnWz/js58+48ttX/LoN4/SL61fdSj0SOmhVzRHoMgNAv8f/6KSiuMPArAOD817CPZugtTOTVydUk3P7XAztN1QhrYbyj1D7mHD/g0syF/AZ1s/45llz/DMsmfIjM/k9MzTOb3j6QxtN5QYV0yoy1bNwNYgEJHxwF+xLk57wRjzWI3XrwKqflIfAn5tjFluZ01VEqOtP/4ndC0BQN+JVhCsfBtG3tOElSllPxGhW0o3uqV044aTbqCwpJDPt37Ogq0LeG/De8xYNwOPw8PgtoMZkTmC0zNPp3NSZ20ttFK2BYGIOIGpwFggH/heRGYbY1YHrLYJOMMYs09EzgGmA6fYVVOgxGhr10+4wzg5C7JHwPI34PTf6tlDKqylx6QzqcckJvWYRJm3jKW7lrJw20IWblvIlMVTmLJ4Ch3iOjAicwQjMkcwrP0w4txxoS5bNRE7WwQnA3nGmI0AIjIDuBCoDgJjTOCQX98AHW2s5yhVh4b2FzdigJD+P4P3b4ftSyFzcBNVplRoRTmjOLXDqZza4VTuGXoP2w9tZ+G2hXy17Ss+3Pghb61/C5e4GNBmAKe0P4Vh7YfRN72vnp4axuwMgkxga8DzfOr/tX898JGN9RwlNc66PcS+4vIT30ifiTDnXljxpgaBarU6xHfgZz1/xs96/owKbwXLCpbx5bYvWbR9EVOXTWXqsqnEueMY3HYwp7Q7hWEdhtE9ubseRgojdgZBbf8KTK0ripyJFQSn1fH6jcCNAFlZTTOaU1UQ7DlUduIbiUmGnuPh329bF5fpvYdUK+d2HulwZjDsK93Hdzu/49sd3/Ltjm/5Iv8LAFKjUzml3Smc0t56dExotsa+OgF2BkE+0CngeUdge82VRKQ/8AJwjjFmT20bMsZMx+o/YMiQIbWGyfGKdjuJj3Kx53AjWgQAA66G1e/B2g+g38VNUZpSYSMlOoWzc87m7JyzAdhxaAff7PiGb3dawfDRZquR3yGuA4PbDq5+ZCdma4uhBbEzCL4HuotIZ2AbcDlwZeAKIpIFvAv8hzFmvY211Cot3sOeQ40Mgm5jIDkbvn9Rg0BFvPbx7bmo+0Vc1P0ijDFsPLCRb3Z8w5JdS/hq+1e8v/F9ANKi0xjUdhCD2w5mSNshdEvupndQDSHbgsAYUykitwAfY50++pIxZpWI3OR/fRrwMJAGPOv/dVBpjBliV001pcZ52NvYFoHDCUOvh3kPw67V0LZP0xSnVJgTEbomd6Vrcleu6n0Vxhg2F21mya4l1Y95W+YBkOBJYGCbgQxuO5hBbQbRJ60PHqceam0uYkyTHGlpNkOGDDGLFy9ukm3d8Pfvyd9Xwtw7RjZuQ4f3wJ97w8Cr4bywHbhNqWa3/dD26lBYunspmw5sAqyL33qn9aZ/en9y2+QyIGMAbWPb6uGkRhCRJXX90I7YK4sB0uKiWJF/oPEbikuDfpNg+esw+j8hNrXx21QqAnSI70CH+A6c3/V8AApLClm+eznLC6zHW+vf4tU1rwLQJqYNuW1yyc3IpX9Gf/qk9SHKGRXK8luNiA6C1Hjr0JAxpvG/NEbcBsv/Cd9OgzMfaJoClYow6THp1TfMA6jwVbB+73qWFSxjRcEKlhcsrz6c5HK46JXSi/4Z/embbg3ck5OYo30NJyCigyAtzkOlz3CgpILk2EYej2zTG3pOgG//BsNvtcYtUEo1itvhtv7Ip/flqt5XAVaroSoUlhcsZ2beTP659p+AdefV3qm96Zvelz5pfeib1pfsxGwcUt/w7Cqig6BNYjQAu4rKGh8EAKffBes+hMUvwYjbG789pdQx0mPSGZ01mtFZowHw+rxsLtrMqj2rWFW4ilV7VvHmujcp81rXCMW546pDoW+aFRAdEzpqOASI6CDokGQFwY4DJfRs1wS/4DsOgS6j4Ku/wuBfQHRi47eplKqX0+GsPjvpgq4XAFDpq2TD/g2s3rOaVXtWsXrPal5b8xoVPuuWMrGuWHqm9qRHSg96pfaiZ0pPuqV0i9i7rUZ0ELSrDoLSptvomIfh+dHw9VNWx7FSqtm5HC56pvakZ2pPLup+EQAV3gry9uexes9q1u1bx7q96/hg4we8se4NABziIDsxm14pveiR2oOeKT3pldqL9Jj0Vn+2UkQHQdvEaERgx/4TGMS+LpmDoe/FsGgqDLkeEts33baVUifM7bROSe2d1rt6mc/42HZoG+v3rmftvrWs27uO5QXLq6+IBut2GT1SetAtuRvdkrvRNbkr3ZK7Ee+JD8Vu2CKig8DtdNAmIYrtTdkiABjzEKx5H+b/F0yc2rTbVko1GYc46JTQiU4JnarPVAIoKi9i/d711S2HdfvW8fb6tyn1Hvlb0S6unRUKSUfCoWty17AcDzqigwCgfVIMO5s6CFK7wKk3W30FA6+C7OFNu32llK0SPYkMaTeEIe2OXH9V1XrYsH8DefvzyNufx4b9G/h+x/eU+47coSAzPrO6z6JrUldyknLIScwhKSopFLsSlIgPgg7J0azdebDpN3zGfbByJrx/B9y0UO9MqlSYC2w9jOo0qnq51+cl/1C+FQ77rHDIO5DHou2LqjunwTrElJOYUx0MVfMdEzqGfCyHiA+C9kkxzF+7G5/P4HA0YYeQJw4mPAH/vBQW/kUHuVeqlXI6nGQnZpOdmM2YrCOHlyp8FWw7uI3NRZvZfGAzm4s2s+nAJhZsXcDe0r3V67nERceEjuQk5dA5sXN1UGQlZpEWndYsHdURHwQ56XGUVvjYWVRKh+QmPnWsxzg46VL4/HHrLqUdm+1+ekqpEHM73NYf9aSco2/IDxwoO3BUQFRNv9r21VGtiDh3HFkJWWQlZpGVkMXwDsOPOlzVVCI+CLqmW+Oubio83PRBAHDuFPjpW3jneusQkV5xrFTES4pKIjfDum9SIK/Py/bD29l8YDM/HfyJn4p+YsvBLazes5p/bfkXDnFoENihS4Z1CtjGgkOM6Jbe9B8QkwwXT4dXzoXZt8ElL+lA90qpWjkdzup+iJoqfBVUeBsxxno9Iv4a67aJUcR6nGwoOGzfh2SfCqMfglXvwkK9TbVS6vi5HW7bTk2N+CAQETqnx7Gp0MYgADjtTuh3CXz6KKydY+9nKaXUcYj4IADo0TaBdXacQhpIBC54GjoMgLd/AZu/svfzlFIqSBoEQN8OiewsKqXgYJm9H+SJhavehuQs+OdlsG2pvZ+nlFJB0CAATsq0rvhbua0JRitrSFw6XPMexKbA/020zihSSqkQ0iAA+mYmIQL/bo4gAEjsANd+ALHp8I8L4cd5zfO5SilVC1uDQETGi8g6EckTkcm1vN5LRBaJSJmI/NbOWuoTH+Wia0Y8P/y0r/k+NCUbrpsL6d3h9cvhu+fBmOb7fKWU8rMtCETECUwFzgH6AFeISJ8aq+0FbgOm2FVHsIZ1SeW7TXup8Pqa70Pj28DPP4RuZ8Gc38J7v4GKJr4BnlJKNcDOFsHJQJ4xZqMxphyYAVwYuIIxZrcx5nvAnqskjsOIrukcLveyfOv+5v3g6ES4/HXrJnXLXoPnz4QdK5q3BqVURLMzCDKBrQHP8/3LjpuI3Cgii0VkcUFBQZMUV9OpXdMQgS9/LLRl+/VyOODMB+DKt6B4jzXC2RdTwKarCJVSKpCdQVDbfRRO6CC4MWa6MWaIMWZIRkZGI8uqXXKsh8FZKcxdudOW7Qelxzi4+RvoNQHmPwrPjYANn4WuHqVURLAzCPI5+p57HYHtNn5eo52f24F1uw7af3FZfWJT4dJX4IoZ4C2zTjF9/UrYuTJ0NSmlWjU7g+B7oLuIdBYRD3A5MNvGz2u0c09qj0Ng5g/bQluICPQ8B27+1rpH0aYvYNoIePMaDQSlVJOzLQiMMZXALcDHwBrgTWPMKhG5SURuAhCRdiKSD9wF/KeI5ItIol01NSQjIYqxfdry+nc/UVxeGaoyjnBHw8jfwh0rYOQ9kDffCoRXzrPGRPa2gBqVUmFPTJiduz5kyBCzePFi27a/ePNeLpm2iIfP68N1p3W27XNOSPFeWPp3+P5FOLAVEjtC7mXQ/zLI6Bnq6pRSLZiILDHG1DqYgQZBDcYYrnrhW1bvKGLBb0eRHNsCxxr2VsL6ubD4Jdj4GRgftOsP/SZZh5TSe+iYB0qpo2gQHKc1O4qY8NSXTBrUkT9dmtvwG0Lp4C5rnIMVb8J2/03sUnKgxznQ/SzoNAyi4kNaolIq9DQITsCUj9fxzGd5PHFpLpMGd7T985rEgXxY/7HVWtj4uXXWkTitW19nj4Cc0yBzCMSlhbpSpVQz0yA4AZVeH1e/+C2LN+9j+jWDGd2rre2f2aTKD8NP38CWr6yxD7YtgapBsZOyrHDoMNCatj0J4u25PkMp1TJoEJygotIKrnr+W9buLOIPF/bjipOzmuVzbVFeDNsWw/Yf/I9lsG/TkddjUiGjl9XpXDVN62bdKdXhDF3dSqkmoUHQCAdKKrj19R/4Yn0BE/q353fn96FNQnSzfb6tivfCjuWwew0UrIWCdda0NOB+Sw63NZBOSrbV95BcNc2yQiIuQ4NCqTCgQdBIXp/huQV5PPVpHh6Xg+tG5HDdaZ1b5hlFjWUMHC6wwmHvRti3GfZvsab7NkNJjVt1ixMS2kFCe0hsDwkdjkwT2lkD8cRlWC0OpysEO6SUAg2CJrOh4BBTPl7HRyt3Eutxcn7/Dlx+cicGdEpGIuV0zdIDsG8L7P8JDu6Aou01pjugvLZbdAjEpFihEJduPWLTjzyPTYOYZGud6GRrPirJuiGfUqrRNAia2JodRbz81SbeX76Dkgov2WmxnN23HWf3bcvATik4HBESCnUpO2gFwqFdVuuieI81PVxY43nBsS2MowhEJ1nhUDMkjgqMBP8jKWA+ATzxGiRK+WkQ2ORgaQUfrNjBnH/vYNGGPVT6DMmxbk7tksapXdMY3jWNrhnxkdNaOBHeSisYivdYfRMl+61wOGZ+n/U8cN54G96+xx8K0YlHh0RUAkQlHhscnjhwxx6Z9/jn3bHgitIL9VTY0iBoBgdKKliwbjdf/ljIog172La/BIC0OA+5nZLJ7ZhM/05J5HZMJjWuFfYtNDdjrJZH6QFrWv0oquV5zWU1HsHeHV2c/nCIqxEYsf5l/sAIXMcTB25/oLhjwBVjTd2x1r2k3AHLtfWibKRB0MyMMWzdW8LXGwpZvGUfK/L38+PuQ9VDEndMiaFXu0R6tUugR7sEerVLoHN6HG6n/iFodj4flB+yAqGi2JovP2ydbls1X9vyimL/8xqPCv/UW378tTij/CERUyM0Ah+x4AoIkJphUjNkXFHW+jWnTo+2biKMBkELcLC0gpXbiliev59/5x9g3a6DbCo8jNdn/fd3O4WuGfF0bRNP57Q4ctLj6JweS05aHKlxHj28FG68FQHhUBUepVBZAhU1HtXLiq11Kor9ywPm61rfNGKM7doConoaXcvzutatbxuBy6KsAHJFWaGnLaBmpUHQQpVWeNlYcJh1u4pYt/MQ63YWsbHwMPn7SqoDAiAh2kXn9Dhy0uLolBpDh+QYMqseKTHEevS0zIhkjBU4FcU1QqNGmFSW+aeB842c+ppgGFVx+kPBbQXDUfMeKzSc/mVVIVIdJLWsVz0fuF4D76nedo3PaYU/vOoLAv0LEkLRbid9OiTSp8PRQzBUeH1s3VvM5j2H2VRYzObCw2zec5ilP+3jw3/vOCokAFJi3dXh0CE5hnZJ0bRJiKJNQjRtEqNokxBFUoxbWxWtjYj1R80Vgj4nn/cEQqTUOmRWWWZNq+crrPtiVfqXef3LqtYrP2SdTFDbelXzwZw4cDwcbn9YuI6EiaNq3u1/ePzrBTx3um14b8B6VadbNzENghbI7XTQJSOeLhnH3jXU6zPsKipl+/4Stu0vIX9fSfX85j2H+SqvkMPlx/5P4XE5/OFwdECkxkWRGucmJdZDWryHlFgPybEenJF+Cqyqn8Pp7ySPDXUlFp/3xEKmvvd4K/yPcqsFFPjcW3FkWWWZ1cdUvazcOhuuapu+yiPvaWxgjbgDxj7SNP/NAmgQhBmnQ+jg/+VfaxsPOFRWye6iUnYfLLMeVfP+aV7BIb7eUEhRae0jnIlAcoyb1DjPUY+UWOuRFOMmMcZFYrSbxBi39TzaTUK0S6+hUKHhcILD31nekvm8R4dIdbDUCIy6Aii9uy1laRC0QvFRLuLraFEEKq3wsq+4nL2Hy9l3uII9h8vYd9h6vrf4yLLNhcUs2bKffcXlxxyWCiRifXZitLs6LKpCIjHGTazHSYzHSazbSWyUi1iP01rmDpj3OIn1WM+jXA49nKVaF4fTf2+ulnW/Mg2CCBbtdtI+KYb2ScH9ivL5DIfLKzlQUkFRSSVFpRX++QqKSisD5v3Tkkq27CmuXl5c4eV4zk1wCMR6XP5wcBLjtsIhyuUkym1No91HnkfXmEa5HET73xMd8N7oGu91uwSP04Hb5bCmToceGlMRxdYgEJHxwF8BJ/CCMeaxGq+L//VzgWLg58aYpXbWpE6cwyEkRLtJiHZDyvG/3xhDWaWP4nIvxeWV/qk1X+KfLyn3ctj/WvWyikoOl3kprfBSVumjrNLLwdJKCivLKav0UlZhLSv1Tyu8jT8TzukQ3E7B7bTCweOyAqJ6WUBoWAEiAetYjyjXkfXd1dsQnI6qqeB2WKHjcgouh8M/FVxOhzUNeM2qyVFd21HPHQ6cVe91WK9pa0oFy7YgEBEnMBUYC+QD34vIbGPM6oDVzgG6+x+nAM/5p6oVEhGi3U6i3U5br672+kx1QJRWB4WvOkgCA6W0wkeF13qUV/oo9/qoqDTVy8oqj7xe4TWU+9erXlZpKC6poKKy5nbMUc8r6zmkZpeqQKgZHlWh4nQIDsE/lYBlVpg4HILTv9yaP3rdwNed/nmHQ3A6ODJfY92qqctZtR2O+eya23KIBDysf0cO8S93VD0/skyEY9av2te6XncELBOxfvQ0tH5VvbW9Hm7sbBGcDOQZYzYCiMgM4EIgMAguBP5hrIsZvhGRZBFpb4zZYWNdqpVzOsTfzxDqSo7w+QwVPitMvF5r3uuzwsKaGrw+Q6XPR6XXUOkzVFa95jN4q94b8J7q9arf4/O/r8b7vdb7q16r8Pnw+QxeY9Xl9Rm8xviXWc99xlRvs6zy6HV9ppb3eKveS/XrNbdnTUP9TTSPY4KlZlA5ag8iwXoe+J7A55cP7cQNp3dp8nrtDIJMYGvA83yO/bVf2zqZgAaBalUcDiHK4SQqwnvljLHCoNLnw+fjSFDUFkb+133GVL/PZ6zl1jL/c/9rR63jf70qhI6sy7Hbq35vbduufX2fzwS3PROwPV9d6x/9ecb/fsPR2zMG0uOjbPle7PxnWVv7qObvgWDWQURuBG4EyMoK4+EilYpwIlWHmHRUu5bEzpt95AOdAp53BLafwDoYY6YbY4YYY4ZkZOgg60op1ZTsDILvge4i0llEPMDlwOwa68wGrhHLMOCA9g8opVTzsu3QkDGmUkRuAT7GOn30JWPMKhG5yf/6NGAO1qmjeVinj/7CrnqUUkrVztauK2PMHKw/9oHLpgXMG+A3dtaglFKqfnpDcKWUinAaBEopFeE0CJRSKsJpECilVIQLu6EqRaQA2HKCb08HCpuwnHCg+xwZdJ8jQ2P2OdsYU+uFWGEXBI0hIovrGrOztdJ9jgy6z5HBrn3WQ0NKKRXhNAiUUirCRVoQTA91ASGg+xwZdJ8jgy37HFF9BEoppY4VaS0CpZRSNURMEIjIeBFZJyJ5IjI51PU0hoi8JCK7RWRlwLJUEZknIj/6pykBr93v3+91InJ2wPLBIvJv/2tPSQsdY09EOonIZyKyRkRWicjt/uWteZ+jReQ7EVnu3+dH/Mtb7T5XERGniPwgIh/4n7fqfRaRzf5al4nIYv+y5t1n4x8tpzU/sO5+ugHoAniA5UCfUNfViP0ZCQwCVgYs+19gsn9+MvC4f76Pf3+jgM7+/w5O/2vfAadiDRD0EXBOqPetjv1tDwzyzycA6/371Zr3WYB4/7wb+BYY1pr3OWDf7wL+CXzQ2v9t+2vdDKTXWNas+xwpLYLq8ZONMeVA1fjJYckY8wWwt8biC4G/++f/DkwMWD7DGFNmjNmEdcvvk0WkPZBojFlkrH9F/wh4T4tijNlhjFnqnz8IrMEa0rQ177MxxhzyP3X7H4ZWvM8AItIRmAC8ELC4Ve9zHZp1nyMlCOoaG7k1aWv8g/r4p238y+va90z/fM3lLZqI5AADsX4ht+p99h8iWQbsBuYZY1r9PgNPAvcCvoBlrX2fDfCJiCzxD8sLzbzPkTKUdlBjI7dSde172P03EZF44B3gDmNMUT2HQFvFPhtjvMAAEUkGZopIv3pWD/t9FpHzgN3GmCUiMiqYt9SyLKz22W+EMWa7iLQB5onI2nrWtWWfI6VFENTYyGFul795iH+627+8rn3P98/XXN4iiYgbKwReM8a861/cqve5ijFmP7AAGE/r3ucRwAUishnr8O1oEXmV1r3PGGO2+6e7gZlYh7KbcN3iYwAAAupJREFUdZ8jJQiCGT853M0GrvXPXwu8F7D8chGJEpHOQHfgO39z86CIDPOfXXBNwHtaFH99LwJrjDF/DnipNe9zhr8lgIjEAGcBa2nF+2yMud8Y09EYk4P1/+h8Y8zVtOJ9FpE4EUmomgfGAStp7n0OdY95cz2wxkZej9XL/mCo62nkvrwO7AAqsH4JXA+kAZ8CP/qnqQHrP+jf73UEnEkADPH/o9sAPIP/AsOW9gBOw2rmrgCW+R/ntvJ97g/84N/nlcDD/uWtdp9r7P8ojpw11Gr3GetMxuX+x6qqv03Nvc96ZbFSSkW4SDk0pJRSqg4aBEopFeE0CJRSKsJpECilVITTIFBKqQinQaCUn4h4/XeArHo02V1qRSRHAu4Wq1RLEim3mFAqGCXGmAGhLkKp5qYtAqUa4L9f/ONijQ/wnYh08y/PFpFPRWSFf5rlX95WRGaKNZbAchEZ7t+UU0SeF2t8gU/8VwwjIreJyGr/dmaEaDdVBNMgUOqImBqHhi4LeK3IGHMy1hWbT/qXPQP8wxjTH3gNeMq//Cngc2NMLta4Eav8y7sDU40xfYH9wCT/8snAQP92brJr55Sqi15ZrJSfiBwyxsTXsnwzMNoYs9F/87udxpg0ESkE2htjKvzLdxhj0kWkAOhojCkL2EYO1q2ku/uf3we4jTH/JSJzgUPALGCWOTIOgVLNQlsESgXH1DFf1zq1KQuY93Kkj24CMBUYDCwREe27U81Kg0Cp4FwWMF3kn/8a6y6ZAFcBC/3znwK/hurBZRLr2qiIOIBOxpjPsAZkSQaOaZUoZSf95aHUETH+EcGqzDXGVJ1CGiUi32L9eLrCv+w24CURuQcoAH7hX347MF1Ersf65f9rrLvF1sYJvCoiSViDi/zFWOMPKNVstI9AqQb4+wiGGGMKQ12LUnbQQ0NKKRXhtEWglFIRTlsESikV4TQIlFIqwmkQKKVUhNMgUEqpCKdBoJRSEU6DQCmlItz/AzZq3e7YWwFkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n",
    "\n",
    "#flatten x matrix from 28x28 to 784x1 vector and add bias to each input set\n",
    "x_train = [k.flatten() for k in trainData]\n",
    "x_train = np.hstack( (np.vstack(np.ones(len(x_train))), x_train) )\n",
    "x_test = [k.flatten() for k in testData]\n",
    "x_test = np.hstack( (np.vstack(np.ones(len(x_test))), x_test) )\n",
    "\n",
    "\n",
    "#Create Weight Vector\n",
    "w = np.zeros(len(x_train[0]))\n",
    "w1, err1 = grad_descent(w, x_train, trainTarget, 0.005, 5000, 0, 10**-7)\n",
    "w2, err2 = grad_descent(w, x_train, trainTarget, 0.001, 5000, 0, 10**-7)\n",
    "w3, err3 = grad_descent(w, x_train, trainTarget, 0.0001, 5000, 0, 10**-7)\n",
    "\n",
    "plt.plot(err1, label='$\\eta$ = 0.005');\n",
    "plt.plot(err2, label='$\\eta$ = 0.001');\n",
    "plt.plot(err3, label='$\\eta$ = 0.0001');\n",
    "plt.ylabel('Training Loss');\n",
    "plt.xlabel('Epochs');\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower left',\n",
    "           ncol=3, mode=\"expand\", borderaxespad=0.)\n",
    "\n",
    "print('Test Loss for \\u03B7 = 0.005: ', crossEntropyLoss(w1, x_test, testTarget, 0))\n",
    "print('Test Loss for \\u03B7 = 0.001: ', crossEntropyLoss(w2, x_test, testTarget, 0))\n",
    "print('Test Loss for \\u03B7 = 0.0001: ', crossEntropyLoss(w3, x_test, testTarget, 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6d913fa578e2f1de303a3500bc7bbb7",
     "grade": false,
     "grade_id": "cell-c6f7f54f2dff0f79",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Briefly discuss the impact of $\\eta$ on the training time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6378c65241ef1cda0ffdcfba2f93af52",
     "grade": true,
     "grade_id": "cell-6e645f7cd4e256f0",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "When increased, $\\eta$ was observed to shorten the training time. Higher values of $\\eta$ increase the change in $\\underline w$ each update as it trends towards the minima. This in turn reduces the number of epochs required to reach an acceptable minima for the training set. \n",
    "\n",
    "Note, increasing $\\eta$ too much may cause the regression to overshoot the minima and oscillate with an unfavorable magnitude. Lowering this value too much will take longer to solve. \n",
    "\n",
    "$\\eta = 0.001$ apeared to be a sweet-spot for training the weights, producing the lowest cross-entropy loss in the test set after 5000 iterations. This implies the weights generalized better (without addition of the generalization term) than $\\eta = 0.005$ and $\\eta = 0.0001$. $\\eta = 0.0001$ still had a high training loss after 5000 iterations, thus did not converge to a good solution compared to $\\eta = 0.001$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f98ad898635a4e20c0ba17d09b766a25",
     "grade": false,
     "grade_id": "cell-0db7ea466ea500bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Generalization [3 points]:\n",
    "Fix the learning rate to $\\eta=0.005$, and consider values for the regularization parameter $\\lambda = 0.001,\\, 0.01,\\, 0.1$. Measure the validation and test losses and state them in your answer below. Comment on the effect of regularization on performance as well as the rationale behind tuning $\\eta$ using the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7c70cce6d267ed7403506522ebda6b51",
     "grade": true,
     "grade_id": "cell-eac467078c174916",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "(Note, the question above asks to comment on the effect of $\\eta$. This should ask to comment on $\\lambda$, which has been done below.) \n",
    "\n",
    "The following table summarizes the validation and test data errors as the three tested values of $\\lambda$. Rather, this is the classification accruacy obntained by computing the cross-entropy-loss for the test and validation data using weights trained with the training data suing varied regularization rates. The cross-entropy loss does not use the regularization for the test and validation data errors. \n",
    "\n",
    "$$\n",
    "\\begin{array} {|r|r|}\\hline \\lambda & Validation\\ Data & Test\\ Data \\\\ \\hline 0.0 & 0.1731 & 0.1503 \\\\ \\hline 0.001 & 0.0974 & 0.0893 \\\\ \\hline 0.01 & 0.0918 & 0.0861 \\\\ \\hline 0.1 & 0.1100 & 0.0989 \\\\ \\hline  \\end{array}\n",
    "$$\n",
    "\n",
    "Tuning the regularization parameter $\\lambda$ optimizes the weights by solving the problem of overfitting. Overfitting implies the weights are trained to best fit the training set, but do not generalize well to create the best solution for all possible datasets. Tuning $\\lambda$ finds the best regularization value to suit all data. \n",
    "\n",
    "$\\lambda$ cab be optimized by observing error in the validation set as well as the training error during training. Overfitting can be observed during the training process by checking the validation dataset errors. If the validation error increases while training error decreases, overfitting may be occurring. A properly tuned $\\lambda$ will ensure both the training and validation errors decrease during training. \n",
    "\n",
    "When training is complete, the weights will be biased to fit both the training and validation dataset. The test set is used independenly of all hyperparameter tuning to assess the error of the trained weights. \n",
    "\n",
    "In this experiment, a sweet spot was observed in selecting $\\lambda$ as seen in the table above. $\\lambda = 0.01$ generalized the training data to best fit the validation and test set better than $\\lambda = 0.001$ and $\\lambda = 0.1$. Less regularization will trend to overfitting the data. Too much regularization may overgeneralize the weights so that they do not provide the lowest average error for all possible data. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
